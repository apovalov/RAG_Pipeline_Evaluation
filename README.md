# RAG pileline & evaluations

1. Build a baseline Retrieval Augmented Generation (RAG) using Llama Index and TrueLens for performance evaluation.
2. A detailed breakdown of methods for evaluating Retrieval Augmented Generation (RAG) systems, including an introduction to the "RAG triad" - a triad of metrics that assess contextual relevance, validity, and response relevance.
3. An advanced Retrieval Augmented Generation (RAG) technique known as Retrieval with suggestion window is considered.
4. Another advanced Retrieval Augmented Generation (RAG) technique called auto-merging is discussed in detail.


![Screenshot 2024-01-27 at 23 38 47](https://github.com/apovalov/advanced_rag_pipeline/assets/43651275/81ff631a-108e-4d62-9dd8-536b290c2c26)

**TrueLens** define a set of metrics so that we can benchmark
**TruEra** evaluation benchmark so that we can measure improvements against the baseline.


### TrueLens Evaluation

![Screenshot 2024-01-27 at 23 42 27](https://github.com/apovalov/advanced_rag_pipeline/assets/43651275/7017189a-c3db-42b2-aeb2-bf1d54a20959)
![Screenshot 2024-01-28 at 20 15 49](https://github.com/apovalov/advanced_rag_pipeline/assets/43651275/17d7084b-7303-40ff-b2a6-195515824a5b)
![Screenshot 2024-01-28 at 21 19 04](https://github.com/apovalov/advanced_rag_pipeline/assets/43651275/d5e1ff2d-7b0b-4601-aa02-30c349b6ad5b)
